#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import numpy as np
import torch
from pathlib import Path
from argparse import ArgumentParser
from datetime import datetime

from utils.easy_renderer_alpha import EasyRenderer   # ä¸æ”¹ easy_renderer
import torchvision

# å˜—è©¦åŒ¯å…¥ imageio åš mp4
try:
    import imageio.v2 as imageio
    HAS_IMAGEIO = True
except ImportError:
    HAS_IMAGEIO = False
    print("[Warn] imageio æœªå®‰è£ï¼Œå°‡åªè¼¸å‡º PNGï¼Œä¸ç”¢ç”Ÿ mp4ï¼ˆå¯ä½¿ç”¨ pip install imageio[ffmpeg]ï¼‰")

# ----------------------------
# COLMAP helpers
# ----------------------------
def read_cameras_txt(path: Path):
    cams = {}
    with open(path, "r") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            toks = line.split()
            cam_id = int(toks[0])
            model  = toks[1].upper()
            width  = int(toks[2])
            height = int(toks[3])
            params = np.array(list(map(float, toks[4:])), dtype=np.float64)
            cams[cam_id] = {
                "model": model,
                "width": width,
                "height": height,
                "params": params
            }
    return cams


def read_images_txt(path: Path):
    imgs = {}
    with open(path, "r") as f:
        lines = [l.strip() for l in f]
    i = 0
    while i < len(lines):
        line = lines[i]
        if not line or line.startswith("#"):
            i += 1
            continue
        toks = line.split()
        # IMAGE_ID qw qx qy qz tx ty tz CAMERA_ID NAME
        q = np.array(list(map(float, toks[1:5])), dtype=np.float64)   # qw qx qy qz
        t = np.array(list(map(float, toks[5:8])), dtype=np.float64)   # tx ty tz
        cam_id = int(toks[8])
        name   = " ".join(toks[9:])
        imgs[name] = {"qvec": q, "tvec": t, "cam_id": cam_id}
        i += 2  # skip 2D points line
    return imgs


def qvec2rotmat(q):
    qw, qx, qy, qz = q
    return np.array([
        [1 - 2*(qy*qy + qz*qz),     2*(qx*qy - qz*qw),     2*(qx*qz + qy*qw)],
        [    2*(qx*qy + qz*qw), 1 - 2*(qx*qx + qz*qz),     2*(qy*qz - qx*qw)],
        [    2*(qx*qz - qy*qw),     2*(qy*qz + qx*qw), 1 - 2*(qx*qx + qy*qy)]
    ], dtype=np.float64)


def rotmat2qvec(R):
    """Convert rotation matrix to quaternion (qw, qx, qy, qz)"""
    q = np.empty(4, dtype=np.float64)
    trace = np.trace(R)
    if trace > 0:
        s = 0.5 / np.sqrt(trace + 1.0)
        q[0] = 0.25 / s
        q[1] = (R[2, 1] - R[1, 2]) * s
        q[2] = (R[0, 2] - R[2, 0]) * s
        q[3] = (R[1, 0] - R[0, 1]) * s
    else:
        if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:
            s = 2.0 * np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2])
            q[0] = (R[2, 1] - R[1, 2]) / s
            q[1] = 0.25 * s
            q[2] = (R[0, 1] + R[1, 0]) / s
            q[3] = (R[0, 2] + R[2, 0]) / s
        elif R[1, 1] > R[2, 2]:
            s = 2.0 * np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2])
            q[0] = (R[0, 2] - R[2, 0]) / s
            q[1] = (R[1, 0] + R[0, 1]) / s
            q[2] = 0.25 * s
            q[3] = (R[1, 2] + R[2, 1]) / s
        else:
            s = 2.0 * np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1])
            q[0] = (R[1, 0] - R[0, 1]) / s
            q[1] = (R[2, 0] + R[0, 2]) / s
            q[2] = (R[2, 1] + R[1, 2]) / s
            q[3] = 0.25 * s
    return q


def intrinsics_from_entry(entry):
    model = entry["model"]
    w, h = entry["width"], entry["height"]
    p = entry["params"]

    if model in ("PINHOLE", "OPENCV", "OPENCV_FISHEYE", "BROWN_CONRADY", "FULL_OPENCV"):
        fx, fy, cx, cy = p[0], p[1], p[2], p[3]
    elif model in ("SIMPLE_PINHOLE", "SIMPLE_RADIAL", "RADIAL"):
        fx = fy = p[0]
        cx = p[1]
        cy = p[2]
    else:
        raise NotImplementedError(f"Unsupported camera model: {model}")

    K = np.array([[fx, 0,  cx],
                  [0,  fy, cy],
                  [0,  0,   1]], dtype=np.float32)
    return K.astype(np.float32), int(h), int(w)

# ----------------------------
# Pose loader
# ----------------------------
def load_poses(
    source_path: str,
    images_subdir: str = "test",
    cameras_subdir: str = "test",
    cameras_filename: str = "cameras.txt",
):
    """
    å¾ source_path/sparse/<images_subdir>/images.txt
    å’Œ source_path/sparse/<cameras_subdir>/<cameras_filename>
    è®€å– COLMAP pose + intrinsicsã€‚
    å›å‚³:
      (w2c, K, H, W, name, qvec, tvec)
    """
    src = Path(source_path)
    imgs_txt = src / "sparse" / images_subdir / "images.txt"
    cams_txt = src / "sparse" / cameras_subdir / cameras_filename

    if not imgs_txt.exists():
        raise FileNotFoundError(f"Missing images.txt: {imgs_txt}")
    if not cams_txt.exists():
        raise FileNotFoundError(f"Missing cameras file: {cams_txt}")

    cams = read_cameras_txt(cams_txt)
    imgs = read_images_txt(imgs_txt)
    cam_ids_sorted = sorted(cams.keys())

    items = []
    for name, meta in imgs.items():
        cam_id = meta["cam_id"]
        if cam_id in cams:
            entry = cams[cam_id]
        else:
            # ä¿éšªè™•ç†ï¼šå¦‚æœ images è£¡çš„ CAMERA_ID è·Ÿ cameras.txt ä¸ match
            if len(cam_ids_sorted) == 1:
                entry = cams[cam_ids_sorted[0]]
            elif 0 <= cam_id < len(cam_ids_sorted):
                entry = cams[cam_ids_sorted[cam_id]]
            else:
                raise KeyError(f"CAMERA_ID {cam_id} not in {cam_ids_sorted}")

        K, H, W = intrinsics_from_entry(entry)

        q = meta["qvec"]
        t = meta["tvec"]
        R = qvec2rotmat(q)  # world->camera
        t_vec = t.reshape(3, 1)
        w2c = np.eye(4, dtype=np.float32)
        w2c[:3, :3] = R.astype(np.float32)
        w2c[:3, 3]  = t_vec[:, 0].astype(np.float32)

        items.append((w2c, K, H, W, name, q, t))
    return items

# ----------------------------
# Find latest model directory
# ----------------------------
_TIMESTAMP_RE = re.compile(r"^\d{8}-\d{6}$")  # e.g., 20251022-205233

def _parse_ts(name: str):
    try:
        return datetime.strptime(name, "%Y%m%d-%H%M%S")
    except Exception:
        return None


def find_latest_model_dir(renderer_base: Path, scene_seq: str) -> Path:
    root = renderer_base / scene_seq
    if not root.exists():
        raise FileNotFoundError(f"Renderer scene root not found: {root}")

    subdirs = [p for p in root.iterdir() if p.is_dir()]
    if not subdirs:
        raise FileNotFoundError(f"No subdirectories under: {root}")

    ts_dirs = [(p, _parse_ts(p.name)) for p in subdirs if _TIMESTAMP_RE.match(p.name)]
    ts_dirs = [(p, ts) for p, ts in ts_dirs if ts is not None]
    if ts_dirs:
        ts_dirs.sort(key=lambda x: x[1], reverse=True)
        return ts_dirs[0][0]

    subdirs.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return subdirs[0]


# ----------------------------
# Quaternion SLERP
# ----------------------------
def slerp_quat(q0, q1, t):
    """
    Spherical linear interpolation between two quaternions.
    q0, q1: (4,) array [qw, qx, qy, qz]
    t: interpolation factor [0, 1]
    """
    dot = np.dot(q0, q1)
    
    # If dot < 0, negate q1 to take shorter path
    if dot < 0.0:
        q1 = -q1
        dot = -dot
    
    # Clamp dot to avoid numerical issues
    dot = np.clip(dot, -1.0, 1.0)
    
    # If quaternions are very close, use linear interpolation
    if dot > 0.9995:
        result = q0 + t * (q1 - q0)
        return result / np.linalg.norm(result)
    
    # Compute the angle between quaternions
    theta_0 = np.arccos(dot)
    theta = theta_0 * t
    
    q2 = q1 - q0 * dot
    q2 = q2 / np.linalg.norm(q2)
    
    return q0 * np.cos(theta) + q2 * np.sin(theta)


def interpolate_poses(q0, t0, q1, t1, alpha):
    """
    åœ¨å…©å€‹ pose ä¹‹é–“æ’å€¼
    q0, q1: quaternion (4,) [qw, qx, qy, qz]
    t0, t1: translation (3,)
    alpha: interpolation factor [0, 1]
    
    è¿”å›: (q_interp, t_interp)
    """
    q_interp = slerp_quat(q0, q1, alpha)
    t_interp = (1 - alpha) * t0 + alpha * t1
    return q_interp, t_interp


# ----------------------------
# Generate interpolated trajectory
# ----------------------------
def smooth_velocity_curve(t, slow_ratio=0.3):
    """
    ç”Ÿæˆå¹³æ»‘çš„é€Ÿåº¦æ›²ç·šï¼Œåœ¨ 0 å’Œ 1 é™„è¿‘æ”¾æ…¢ï¼Œä¸­é–“ä¿æŒè¼ƒå¿«é€Ÿåº¦
    
    Args:
        t: è¼¸å…¥åƒæ•¸ [0, 1]
        slow_ratio: åœ¨å…©ç«¯æ”¾æ…¢çš„å€åŸŸæ¯”ä¾‹ï¼ˆ0-0.5ï¼‰
    
    Returns:
        å¹³æ»‘æ˜ å°„å¾Œçš„å€¼ [0, 1]
    """
    if slow_ratio <= 0:
        # å®Œå…¨ç·šæ€§ï¼Œç„¡é€Ÿåº¦è®ŠåŒ–
        return t
    
    if t <= slow_ratio:
        # å‰æ®µï¼šä½¿ç”¨ smoothstep (3t^2 - 2t^3) æ›´å¹³æ»‘
        normalized = t / slow_ratio
        smoothed = normalized * normalized * (3 - 2 * normalized)
        return 0.5 * slow_ratio * smoothed
    elif t >= (1 - slow_ratio):
        # å¾Œæ®µï¼šä½¿ç”¨ smoothstep
        normalized = (t - (1 - slow_ratio)) / slow_ratio
        smoothed = normalized * normalized * (3 - 2 * normalized)
        return 1 - 0.5 * slow_ratio * (1 - smoothed)
    else:
        # ä¸­æ®µï¼šç·šæ€§ç§»å‹•ï¼ˆè¼ƒå¿«ï¼‰
        mid_start = 0.5 * slow_ratio
        mid_end = 1 - 0.5 * slow_ratio
        mid_length = mid_end - mid_start
        
        progress = (t - slow_ratio) / (1 - 2 * slow_ratio)
        return mid_start + progress * mid_length


def generate_interpolated_trajectory(keyframes, min_total_frames=150, 
                                     frames_per_segment=None, slow_ratio=0.15):
    """
    åœ¨é—œéµ frame ä¹‹é–“ç”Ÿæˆå¹³æ»‘å…§æ’è»Œè·¡ï¼ˆæ”¹é€²ç‰ˆï¼šæ›´è‡ªç„¶çš„é€Ÿåº¦è®ŠåŒ–ï¼‰
    
    Args:
        keyframes: list of (q, t, name) - é—œéµå¹€çš„ pose
        min_total_frames: æœ€å°ç¸½å¹€æ•¸
        frames_per_segment: æ¯æ®µçš„å¹€æ•¸ï¼ˆNone å‰‡è‡ªå‹•è¨ˆç®—ï¼‰
        slow_ratio: åœ¨é—œéµå¹€é™„è¿‘æ”¾æ…¢çš„å€åŸŸæ¯”ä¾‹ï¼ˆ0-0.5ï¼Œé è¨­ 0.15ï¼›è¨­ç‚º 0 å‰‡å®Œå…¨å‹»é€Ÿï¼‰
    
    Returns:
        list of (q, t, frame_type, keyframe_idx, frame_name) 
    """
    if len(keyframes) < 2:
        raise ValueError("è‡³å°‘éœ€è¦ 2 å€‹é—œéµå¹€")
    
    n_segments = len(keyframes) - 1
    
    # è‡ªå‹•è¨ˆç®—æ¯æ®µéœ€è¦å¤šå°‘å¹€
    if frames_per_segment is None:
        frames_per_segment = max(30, min_total_frames // n_segments)
    
    trajectory = []
    use_constant_speed = (slow_ratio < 0.01)  # å¹¾ä¹ç‚º 0 æ™‚ä½¿ç”¨å®Œå…¨å‹»é€Ÿ
    
    for i in range(n_segments):
        q_curr, t_curr, name_curr = keyframes[i]
        q_next, t_next, name_next = keyframes[i + 1]
        
        # ç”Ÿæˆé€™ä¸€æ®µçš„æ‰€æœ‰å¹€
        for j in range(frames_per_segment):
            # ç·šæ€§é€²åº¦ [0, 1]
            t_linear = j / frames_per_segment
            
            # æ‡‰ç”¨å¹³æ»‘é€Ÿåº¦æ›²ç·šï¼ˆæˆ–ä½¿ç”¨å‹»é€Ÿï¼‰
            if use_constant_speed:
                t_smooth = t_linear  # å®Œå…¨å‹»é€Ÿ
                frame_type = 'linear'
            else:
                t_smooth = smooth_velocity_curve(t_linear, slow_ratio)
                # åˆ¤æ–·å¹€é¡å‹ï¼ˆç”¨æ–¼ debugï¼‰
                if j == 0:
                    frame_type = 'key'
                elif t_linear < slow_ratio or t_linear > (1 - slow_ratio):
                    frame_type = 'slow'
                else:
                    frame_type = 'fast'
            
            # æ’å€¼ pose
            q_interp, t_interp = interpolate_poses(q_curr, t_curr, q_next, t_next, t_smooth)
            
            if j == 0:
                frame_name = name_curr
            else:
                frame_name = f"{name_curr}_to_{name_next}"
            
            trajectory.append((q_interp, t_interp, frame_type, i, frame_name))
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹é—œéµå¹€
    q_last, t_last, name_last = keyframes[-1]
    trajectory.append((q_last, t_last, 'key', n_segments, name_last))
    
    # æª¢æŸ¥ç¸½å¹€æ•¸ï¼Œå¦‚æœä¸å¤ å°±å¢åŠ æ¯æ®µçš„å¹€æ•¸
    current_frames = len(trajectory)
    if current_frames < min_total_frames:
        needed_per_segment = (min_total_frames - 1) // n_segments + 1
        print(f"ç•¶å‰å¹€æ•¸ {current_frames} < {min_total_frames}ï¼Œå¢åŠ æ¯æ®µå¹€æ•¸åˆ° {needed_per_segment}")
        return generate_interpolated_trajectory(keyframes, min_total_frames, 
                                               needed_per_segment, slow_ratio)
    
    return trajectory


# ----------------------------
# Main
# ----------------------------
def main():
    ap = ArgumentParser()
    ap.add_argument("--source_path", required=True,
                    help="è³‡æ–™æ ¹ç›®éŒ„ï¼ˆåº•ä¸‹æœ‰ sparse/ å’Œ images_test_select/ï¼‰")

    # é è¨­è®€ test
    ap.add_argument("--images_subdir", default="test",
                    help="è®€ images.txt çš„å­è³‡æ–™å¤¾ï¼ˆé è¨­ sparse/testï¼‰")
    ap.add_argument("--cameras_subdir", default="test",
                    help="è®€ cameras çš„å­è³‡æ–™å¤¾ï¼ˆé è¨­ sparse/testï¼‰")
    ap.add_argument("--cameras_filename", default="cameras.txt",
                    help="cameras æª”åï¼ˆé è¨­ cameras.txtï¼‰")
    
    # ç¯©é¸åœ–ç‰‡çš„è³‡æ–™å¤¾
    ap.add_argument("--select_dir", default="images_test_select",
                    help="ç¯©é¸å¾Œçš„åœ–ç‰‡è³‡æ–™å¤¾åç¨±ï¼ˆé è¨­ images_test_selectï¼‰")
    
    # æ¨¡å‹ä¾†æºï¼ˆGraphDECO / EasyRenderer æ¨¡å‹ï¼‰
    ap.add_argument("--model_path", default=None,
                    help="ç›´æ¥æŒ‡å®š EasyRenderer æ¨¡å‹è³‡æ–™å¤¾ï¼ˆè¦†è“‹è‡ªå‹•æœå°‹ï¼‰")
    ap.add_argument("--renderer_base", default=None,
                    help="æ¨¡å‹æ ¹ç›®éŒ„ï¼ˆåº•ä¸‹æœ‰ <scene_seq>/<timestamp>ï¼‰")

    # scene_seq
    ap.add_argument("--scene_seq", default=None,
                    help="scene/seq åç¨±ï¼ŒæœªæŒ‡å®šå‰‡å– source_path çš„æœ€å¾Œä¸€å±¤ç›®éŒ„å")

    ap.add_argument("--iteration", type=int, default=10000,
                    help="EasyRenderer è¦è¼‰å…¥çš„ iterationï¼ˆå°æ‡‰ä½ è¨“ç·´å¥½çš„ ckptï¼‰")

    # RGB è¼¸å‡ºè³‡æ–™å¤¾
    ap.add_argument("--out_dir", default=None,
                    help="render RGB è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆæœªæŒ‡å®šå‰‡ä¸Ÿåˆ° model_path/test_pack_interpolatedï¼‰")

    # mp4 fps
    ap.add_argument("--fps", type=int, default=30,
                    help="è¼¸å‡º mp4 çš„ FPSï¼ˆé è¨­ 30ï¼‰")

    # å…§æ’åƒæ•¸
    ap.add_argument("--min_frames", type=int, default=200,
                    help="æœ€å°ç¸½å¹€æ•¸ï¼ˆé è¨­ 200ï¼Œå¢åŠ å¯è®“é€Ÿåº¦è®ŠåŒ–æ›´ç´°è†©ï¼‰")
    ap.add_argument("--frames_per_segment", type=int, default=None,
                    help="æ¯æ®µçš„å¹€æ•¸ï¼ˆNone å‰‡æ ¹æ“š min_frames è‡ªå‹•è¨ˆç®—ï¼‰")
    ap.add_argument("--slow_ratio", type=float, default=0.15,
                    help="åœ¨é—œéµå¹€é™„è¿‘æ”¾æ…¢çš„å€åŸŸæ¯”ä¾‹ [0-0.5]ï¼ˆé è¨­ 0.15ï¼Œå³å‰å¾Œå„ 15%% æœƒæ”¾æ…¢ï¼›è¨­ç‚º 0 å‰‡å®Œå…¨å‹»é€Ÿï¼‰")

    # Floater éæ¿¾åƒæ•¸
    ap.add_argument("--filter_floaters", action="store_true",
                    help="å•Ÿç”¨ floater éæ¿¾ï¼ˆç§»é™¤é£„æµ®çš„ Gaussiansï¼‰")
    ap.add_argument("--opacity_threshold", type=float, default=0.1,
                    help="é€æ˜åº¦é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼çš„ Gaussians æœƒè¢«éæ¿¾ï¼ˆé è¨­ 0.1ï¼‰")
    ap.add_argument("--scale_threshold", type=float, default=None,
                    help="Scale é–¾å€¼ï¼Œå¤§æ–¼æ­¤å€¼çš„ Gaussians æœƒè¢«éæ¿¾ï¼ˆé è¨­ None ä¸éæ¿¾ï¼‰")
    ap.add_argument("--depth_near", type=float, default=None,
                    help="æœ€è¿‘æ·±åº¦ï¼Œå°æ–¼æ­¤å€¼çš„é»æœƒè¢«éæ¿¾ï¼ˆé è¨­ Noneï¼‰")
    ap.add_argument("--depth_far", type=float, default=None,
                    help="æœ€é æ·±åº¦ï¼Œå¤§æ–¼æ­¤å€¼çš„é»æœƒè¢«éæ¿¾ï¼ˆé è¨­ Noneï¼‰")

    args = ap.parse_args()
    
    # è§£æ scene_seq
    src = Path(args.source_path).resolve()
    if args.scene_seq is not None:
        scene_seq = args.scene_seq
    else:
        if len(src.parts) >= 1:
            scene_seq = src.parts[-1]
        else:
            raise RuntimeError("ç„¡æ³•è‡ªå‹•æ¨å° scene_seqï¼Œè«‹ç”¨ --scene_seq æŒ‡å®š")

    # è§£æ model_path
    if args.model_path is not None:
        model_path = Path(args.model_path).resolve()
        if not model_path.exists():
            raise FileNotFoundError(f"--model_path ä¸å­˜åœ¨: {model_path}")
    else:
        if args.renderer_base is None:
            raise ValueError("è«‹æä¾› --renderer_base æˆ–ç›´æ¥æŒ‡å®š --model_path")
        renderer_base = Path(args.renderer_base).resolve()
        model_path = find_latest_model_dir(renderer_base, scene_seq)
    print(f"[EasyRenderer RGB] model_path={model_path}  iteration={args.iteration}")

    # åˆå§‹åŒ– renderer
    er = EasyRenderer(model_path=str(model_path), iteration=args.iteration)

    # Floater éæ¿¾
    if args.filter_floaters:
        print("\n" + "="*50)
        print("[Floater Filtering] Enabled")
        print("="*50)
        
        try:
            # æ–¹æ³• 1ï¼šä½¿ç”¨ç°¡å–®çš„éæ¿¾ï¼ˆæ¨è–¦ï¼‰
            original_count = len(er.gaussians.get_xyz)
            
            # å»ºç«‹éæ¿¾ mask
            mask = torch.ones(original_count, dtype=torch.bool, device=er.gaussians.get_xyz.device)
            
            # é€æ˜åº¦éæ¿¾
            if args.opacity_threshold > 0:
                opacity = er.gaussians.get_opacity.squeeze()
                opacity_mask = opacity >= args.opacity_threshold
                filtered = (~opacity_mask).sum().item()
                mask = mask & opacity_mask
                print(f"  âœ“ Opacity filter: removed {filtered} low-opacity Gaussians (threshold={args.opacity_threshold})")
            
            # Scale éæ¿¾
            if args.scale_threshold is not None:
                scaling = er.gaussians.get_scaling
                max_scale = scaling.max(dim=1)[0]
                scale_mask = max_scale <= args.scale_threshold
                filtered = (~scale_mask).sum().item()
                mask = mask & scale_mask
                print(f"  âœ“ Scale filter: removed {filtered} large Gaussians (threshold={args.scale_threshold})")
            
            # æ·±åº¦éæ¿¾
            if args.depth_near is not None or args.depth_far is not None:
                xyz = er.gaussians.get_xyz
                scene_center = xyz.mean(dim=0)
                distances = torch.norm(xyz - scene_center, dim=1)
                
                if args.depth_near is not None:
                    near_mask = distances >= args.depth_near
                    filtered = (~near_mask).sum().item()
                    mask = mask & near_mask
                    print(f"  âœ“ Near depth filter: removed {filtered} Gaussians (threshold={args.depth_near})")
                
                if args.depth_far is not None:
                    far_mask = distances <= args.depth_far
                    filtered = (~far_mask).sum().item()
                    mask = mask & far_mask
                    print(f"  âœ“ Far depth filter: removed {filtered} Gaussians (threshold={args.depth_far})")
            
            # çµ±è¨ˆ
            total_filtered = (~mask).sum().item()
            remaining = mask.sum().item()
            
            if total_filtered > 0:
                print(f"\n  ğŸ“Š Summary:")
                print(f"     Original:  {original_count:,} Gaussians")
                print(f"     Filtered:  {total_filtered:,} ({total_filtered/original_count*100:.1f}%)")
                print(f"     Remaining: {remaining:,} Gaussians")
                
                # æ‡‰ç”¨éæ¿¾ï¼ˆä¿®æ”¹ Gaussiansï¼‰
                # æ³¨æ„ï¼šé€™å¯èƒ½éœ€è¦æ ¹æ“š EasyRenderer çš„å¯¦éš›å¯¦ä½œèª¿æ•´
                try:
                    er.gaussians._xyz = er.gaussians._xyz[mask]
                    er.gaussians._features_dc = er.gaussians._features_dc[mask]
                    er.gaussians._features_rest = er.gaussians._features_rest[mask]
                    er.gaussians._scaling = er.gaussians._scaling[mask]
                    er.gaussians._rotation = er.gaussians._rotation[mask]
                    er.gaussians._opacity = er.gaussians._opacity[mask]
                    print(f"  âœ… Filtering applied successfully!\n")
                except AttributeError as e:
                    print(f"  âš ï¸  Warning: Cannot directly modify Gaussians")
                    print(f"     Error: {e}")
                    print(f"     The renderer may not support in-place filtering.")
                    print(f"     Rendering will proceed with unfiltered Gaussians.\n")
            else:
                print(f"  â„¹ï¸  No Gaussians were filtered with current thresholds\n")
                
        except Exception as e:
            print(f"  âŒ Error during filtering: {e}")
            print(f"     Rendering will proceed with unfiltered Gaussians.\n")
    else:
        print("\n[Floater Filtering] Disabled")
        print("  ğŸ’¡ Use --filter_floaters to enable floater removal")
        print("  ğŸ’¡ Typical usage: --filter_floaters --opacity_threshold 0.1 --scale_threshold 0.5\n")


    # è¼‰å…¥æ‰€æœ‰ posesï¼ˆimages.txt + cameras.txtï¼‰
    all_items = load_poses(
        source_path=str(src),
        images_subdir=args.images_subdir,
        cameras_subdir=args.cameras_subdir,
        cameras_filename=args.cameras_filename,
    )
    print(f"Loaded {len(all_items)} poses from {src}/sparse/{args.images_subdir}/images.txt")

    # æª¢æŸ¥ images_test_select è³‡æ–™å¤¾ï¼Œæ‰¾å‡ºå¯¦éš›å­˜åœ¨çš„åœ–ç‰‡
    select_dir = src / args.select_dir
    if not select_dir.exists():
        raise FileNotFoundError(f"ç¯©é¸åœ–ç‰‡è³‡æ–™å¤¾ä¸å­˜åœ¨: {select_dir}")
    
    # ç²å–æ‰€æœ‰ç¯©é¸å¾Œçš„åœ–ç‰‡æª”å
    selected_images = set()
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:
        selected_images.update([p.name for p in select_dir.glob(ext)])
    
    print(f"Found {len(selected_images)} selected images in {select_dir}")

    # ç¯©é¸å‡ºå­˜åœ¨æ–¼ images_test_select ä¸­çš„ pose
    filtered_items = []
    for item in all_items:
        w2c, K, H, W, name, q, t = item
        if name in selected_images:
            filtered_items.append(item)
    
    if len(filtered_items) < 2:
        raise RuntimeError(f"ç¯©é¸å¾Œè‡³å°‘éœ€è¦ 2 å€‹ poseï¼Œä½†åªæ‰¾åˆ° {len(filtered_items)} å€‹")
    
    # æŒ‰æª”åæ’åº
    filtered_items = sorted(filtered_items, key=lambda x: x[4])
    print(f"Filtered to {len(filtered_items)} poses that exist in {args.select_dir}/")
    print("Selected keyframes:")
    for idx, (_, _, _, _, name, _, _) in enumerate(filtered_items):
        print(f"  {idx}: {name}")

    # intrinsics / resolution ç”¨ç¬¬ä¸€å€‹ viewï¼Œä¸¦æŠŠ FOV æ‹‰å»£
    w2c0, K_base, H_base, W_base, name0, q0, t0 = filtered_items[0]
    K = K_base.copy().astype(np.float32)
    K[0, 0] *= 0.8  # fx
    K[1, 1] *= 0.8  # fy
    H, W = H_base, W_base
    print(f"\nBase view (for intrinsics): {name0}")
    print(f"Scaled intrinsics: fx={K[0,0]:.2f}, fy={K[1,1]:.2f}")

    # æº–å‚™é—œéµå¹€è³‡æ–™
    keyframes = [(item[5], item[6], item[4]) for item in filtered_items]  # (q, t, name)
    
    # ç”Ÿæˆå…§æ’è»Œè·¡
    print(f"\nGenerating interpolated trajectory...")
    print(f"  slow_ratio={args.slow_ratio} (å‰å¾Œå„ {args.slow_ratio*100:.0f}% å€åŸŸæœƒæ”¾æ…¢)")
    if args.frames_per_segment:
        print(f"  frames_per_segment={args.frames_per_segment}")
    trajectory = generate_interpolated_trajectory(
        keyframes, 
        min_total_frames=args.min_frames,
        frames_per_segment=args.frames_per_segment,
        slow_ratio=args.slow_ratio
    )
    
    total_frames = len(trajectory)
    print(f"Generated {total_frames} frames (target: >={args.min_frames})")

    # è¼¸å‡ºè³‡æ–™å¤¾
    if args.out_dir is None:
        out_dir = model_path / "test_pack_interpolated"
    else:
        out_dir = Path(args.out_dir).resolve()
    png_dir = out_dir / "png"
    png_dir.mkdir(parents=True, exist_ok=True)
    out_dir.mkdir(parents=True, exist_ok=True)

    # æ ¹æ“š out_dir è·¯å¾‘æ±ºå®šå½±ç‰‡åç¨±
    video_name = "video.mp4"
    parts = set(out_dir.parts)
    if "baseline" in parts:
        video_name = "3dgs.mp4"
    elif "ours" in parts:
        video_name = "ours.mp4"

    print(f"\nPNG will be saved to : {png_dir}")
    print(f"MP4 will be saved to : {out_dir / video_name}")
    print(f"Video duration: ~{total_frames/args.fps:.1f} seconds @ {args.fps} fps\n")

    frames_for_video = []

    # æ¸²æŸ“æ‰€æœ‰å¹€
    for idx, (q, t, frame_type, key_idx, frame_name) in enumerate(trajectory):
        # å¾ quaternion + translation æ§‹å»º w2c
        R = qvec2rotmat(q)
        w2c = np.eye(4, dtype=np.float32)
        w2c[:3, :3] = R.astype(np.float32)
        w2c[:3, 3] = t.astype(np.float32)

        # æ¸²æŸ“
        rgb, alpha_map, depth = er.render(w2c, K, H, W)
        rgb = rgb.clamp(0, 1).float()

        # å­˜ PNG
        frame_filename = f"frame_{idx:04d}.png"
        frame_path = png_dir / frame_filename
        torchvision.utils.save_image(rgb.cpu(), str(frame_path))

        # æ”¶é›†çµ¦ mp4
        if HAS_IMAGEIO:
            frame = (rgb.cpu().numpy().transpose(1, 2, 0) * 255.0)
            frame = np.clip(frame, 0, 255).astype(np.uint8)
            frames_for_video.append(frame)

        # é€²åº¦é¡¯ç¤º
        if idx % 20 == 0 or idx == total_frames - 1:
            type_str = {'key': 'KEY', 'slow': 'SLOW', 'fast': 'FAST', 'linear': 'LINEAR'}[frame_type]
            print(f"Rendered {idx+1}/{total_frames} [{type_str}] segment={key_idx} -> {frame_filename}")

    # è¼¸å‡º mp4
    if HAS_IMAGEIO and len(frames_for_video) > 0:
        video_path = out_dir / video_name
        imageio.mimsave(video_path, frames_for_video, fps=args.fps)
        duration = len(frames_for_video) / args.fps
        print(f"\n[Video] Saved mp4 to: {video_path}")
        print(f"        Duration: {duration:.2f} seconds ({len(frames_for_video)} frames @ {args.fps} fps)")
    elif not HAS_IMAGEIO:
        print("\n[Video] è·³é mp4 è¼¸å‡ºï¼Œå› ç‚ºæ²’æœ‰å®‰è£ imageioï¼ˆpip install imageio[ffmpeg]ï¼‰")
    else:
        print("\n[Video] æ²’æœ‰ä»»ä½• frameï¼Œè¢«è·³éã€‚")

    print(f"\nDone! Saved {total_frames} PNG images to: {png_dir}")
    print("\nTrajectory summary:")
    print(f"  Total frames: {total_frames}")
    print(f"  Keyframes: {len(keyframes)}")
    print(f"  Segments: {len(keyframes) - 1}")
    print(f"  Avg frames per segment: {total_frames / (len(keyframes) - 1):.1f}")
    print(f"  Slow ratio: {args.slow_ratio} (å‰å¾Œå„ {args.slow_ratio*100:.0f}% æœƒæ”¾æ…¢)")

if __name__ == "__main__":
    main()